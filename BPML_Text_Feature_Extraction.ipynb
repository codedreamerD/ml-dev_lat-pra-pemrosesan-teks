{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzFs3B/OM+X18b/m9wMvHc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codedreamerD/ml-dev_latihan/blob/main/BPML_Text_Feature_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEXT FEATURE EXTRACTION"
      ],
      "metadata": {
        "id": "qrVwGddi9PBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ],
      "metadata": {
        "id": "b-fLnjsC9ceL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfNumtYx7um0",
        "outputId": "d0db679c-c501-4585-ba84-6ece20015fb4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: numpy\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed numpy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "615998Ul7TjE"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.util import ngrams"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embedding\n",
        "\n",
        "Bayangkan Anda lagi main puzzle kata-kata. Anda punya sejumlah kata-kata yang tersusun dalam kalimat-kalimat, tetapi bagaimana cara mesin memahami makna sebenarnya dari kata-kata ini?\n",
        "\n",
        "Nah, di situlah \"word embedding\" masuk ke permainan. Hal ini seperti memberi setiap kata-kata tempat spesial dalam ruang matematis yang besar. Bayangkan ruang itu seperti tempat parkir yang luas dan setiap kata punya tempat parkirnya sendiri.\n",
        "\n",
        "Contohnya, kata-kata yang sering digunakan bersama-sama, seperti \"anjing\" dan \"kucing\", parkirnya dekat satu sama lain karena sering muncul dalam konteks yang sama. Sementara kata-kata seperti \"meja\" atau \"laptop\" mungkin parkirnya lebih jauh dari mereka karena maknanya berada dalam konteks berbeda.\n",
        "\n",
        "Kode berikut adalah contoh implementasi Word2Vec menggunakan library Gensim dan NLTK di Python."
      ],
      "metadata": {
        "id": "VWvauyjg-Ke3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Membangun model Word2Vec menggunakan data teks yang sudah di-tokenisasi"
      ],
      "metadata": {
        "id": "6sYHWldC9fxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Kemudian, kita men-download modul Punkt tokenizer dari NLTK.\n",
        "- Langkah selanjutnya adalah mendefinisikan contoh data teks yang akan kita gunakan untuk melatih model Word2Vec.\n",
        "- Setelah itu, kita melakukan tokenisasi pada teks tersebut.\n",
        "- Sekarang, saatnya membangun model Word2Vec menggunakan data teks yang sudah di-tokenisasi.\n",
        "- Setelah model dibangun, kita bisa menggunakan vektor kata untuk kata-kata tertentu atau mencari kata-kata yang mirip dengan kata tertentu."
      ],
      "metadata": {
        "id": "niZVLd8IGj4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text_data = [\n",
        "    'Saya suka makan bakso',\n",
        "    'Bakso enak dan lezat',\n",
        "    'Makanan favorit saya adalah nasi goreng',\n",
        "    'Nasi goreng pedas adalah makanan favorit saya',\n",
        "    'Saya suka makanan manis seperti es krim',\n",
        "]\n",
        "\n",
        "tokenized_data = [word_tokenize(sentence.lower()) for sentence in text_data]\n",
        "\n",
        "model = Word2Vec(sentences=tokenized_data,\n",
        "                 vector_size=100,\n",
        "                 window=5,\n",
        "                 min_count=1,\n",
        "                 workers=4)\n",
        "\n",
        "word_vectors = model.wv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7IZRgxU7o3O",
        "outputId": "9bb8f356-ce2d-4d2c-a246-442df40288b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Menggunakan vektor kata untuk kata-kata tertentu atau mencari kata-kata yang mirip dengan kata tertentu"
      ],
      "metadata": {
        "id": "wJ9iZMZv9rac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dalam contoh ini, kita mencari kata-kata yang mirip dengan 'bakso' dan mendapatkan vektor representasinya. Jadi, dengan menggunakan Word2Vec, kita bisa melatih model untuk membuat representasi vektor dari kata-kata dalam teks yang berguna pada berbagai tugas NLP."
      ],
      "metadata": {
        "id": "jBz0YZ_T94bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similar_words = word_vectors.most_similar('bakso', topn=3)\n",
        "print(\"Kata-kata yang mirip dengan 'bakso':\", similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnELdeRr7ozu",
        "outputId": "d31bc501-82f6-47b9-d396-2faa4a320652"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kata-kata yang mirip dengan 'bakso': [('manis', 0.2529163062572479), ('nasi', 0.17018672823905945), ('enak', 0.15006466209888458)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector = word_vectors['bakso']\n",
        "print(\"Vektor untuk 'bakso':\", vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_I-VW6P7oxQ",
        "outputId": "2669f516-9d0b-46df-994b-67a298b3bb3c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vektor untuk 'bakso': [-0.00713882  0.00124156 -0.00717766 -0.00224369  0.00371885  0.00583258\n",
            "  0.00119832  0.00210183 -0.00411138  0.00722588 -0.00630644  0.00464789\n",
            " -0.00821918  0.00203677 -0.00497649 -0.00424685 -0.00310906  0.00565491\n",
            "  0.00579776 -0.00497439  0.00077378 -0.0084959   0.00780977  0.00925648\n",
            " -0.00274235  0.0007995   0.00074748  0.00547704 -0.00860589  0.00058358\n",
            "  0.00687019  0.00223141  0.00112457 -0.00932216  0.00848288 -0.0062632\n",
            " -0.00299165  0.00349458 -0.00077282  0.00141124  0.00178217 -0.00682961\n",
            " -0.00972456  0.00904072  0.00619895 -0.00691193  0.00340259  0.00020664\n",
            "  0.00475438 -0.00712046  0.00402629  0.00434812  0.00995727 -0.00447314\n",
            " -0.00138943 -0.00731689 -0.00969748 -0.00908048 -0.00102362 -0.00650396\n",
            "  0.0048507  -0.00616346  0.0025184   0.00073924 -0.00339173 -0.00097928\n",
            "  0.00997817  0.009146   -0.00446089  0.00908287 -0.00564239  0.00593029\n",
            " -0.00309763  0.00343232  0.00301726  0.00690047 -0.00237434  0.00877584\n",
            "  0.00759023 -0.00954767 -0.00800735 -0.00763848  0.0029233  -0.00279572\n",
            " -0.00692899 -0.00812822  0.0083084   0.0019909  -0.00932751 -0.00479288\n",
            "  0.00313647 -0.00471295  0.0052802  -0.00423267  0.00264146 -0.00804574\n",
            "  0.00620901  0.00481829  0.00078651  0.00301266]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight:**\n",
        "\n",
        "Hasil yang kita dapatkan dari kode tersebut adalah berupa dua hal, yaitu kata-kata yang mirip dengan 'bakso' dan vektor representasi untuk kata 'bakso'.\n",
        "\n",
        "Kata-kata yang mirip dengan 'bakso'\n",
        "Dari hasil pencarian, kita dapatkan tiga kata yang mirip dengan 'bakso', yaitu 'manis', 'nasi', dan 'enak'. Nilai yang tercantum di samping kata-kata tersebut adalah ukuran seberapa miripnya kata-kata tersebut dengan 'bakso'. Semakin tinggi nilai tersebut, semakin mirip kata-kata tersebut dengan 'bakso'. Jadi, dalam kasus ini, 'manis' adalah kata yang paling mirip dengan 'bakso', diikuti oleh 'nasi' dan 'enak'.\n",
        "Vektor untuk 'bakso'\n",
        "Vektor ini adalah representasi matematis dari kata 'bakso' dalam ruang vektor. Setiap angka dalam vektor tersebut mewakili fitur atau atribut tertentu dari kata 'bakso'.\n",
        "\n",
        "Misalnya, nilai pertama mungkin mewakili seberapa sering kata 'bakso' muncul dalam konteks tertentu, nilai kedua mungkin mewakili hubungan kata 'bakso' dengan kata-kata lain, seperti 'makanan' atau 'kuliner' dan seterusnya. Dengan vektor ini, mesin dapat memahami kata 'bakso' secara matematis dan menggunakannya dalam berbagai analisis atau tugas pemrosesan bahasa alami."
      ],
      "metadata": {
        "id": "mZf6vjay9-gu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "\n",
        "Bayangkan Anda memiliki sejumlah dokumen teks, seperti artikel, laporan, atau pesan-pesan dalam media sosial. Anda ingin tahu kata-kata yang penting dalam setiap dokumen itu. Misalnya, dalam artikel tentang makanan, kata-kata seperti \"makanan\" atau \"resep\" mungkin lebih penting daripada kata-kata umum lainnya.\n",
        "\n",
        "Di sinilah TF-IDF (Term Frequency-Inverse Document Frequency) masuk permainan. Ini seperti memberi bobot kepada setiap kata dalam dokumen berdasarkan seberapa sering kata itu muncul pada dokumen itu sendiri (frekuensi kata) dan seberapa umumnya kata itu muncul di seluruh kumpulan dokumen.\n",
        "\n",
        "Misalnya, kata-kata yang muncul banyak dalam satu dokumen, tetapi jarang muncul pada dokumen-dokumen lain mungkin dianggap lebih penting. Contohnya, dalam sebuah artikel tentang bakso, kata \"bakso\" mungkin muncul berkali-kali, tetapi mungkin jarang muncul pada artikel-artikel yang tidak berkaitan dengan makanan.\n",
        "\n",
        "Untuk menghitung TF-IDF, kita bisa menggunakan alat seperti TfidfVectorizer pada Python. Ini membantu kita mengukur pentingnya kata-kata dalam setiap dokumen berdasarkan logika yang dibahas tadi. Jadi, dengan TF-IDF, kita bisa menemukan kata-kata yang paling mencirikan atau penting dalam setiap dokumen.\n",
        "\n",
        "Kode berikut adalah contoh implementasi TF-IDF menggunakan library TfidfVectorizer pada Python."
      ],
      "metadata": {
        "id": "a4TpTNMz-cAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inisialisasi objek TfidfVectorizer dan menghitung TF-IDF dari dokumen-dokumen tersebut"
      ],
      "metadata": {
        "id": "wn1whI9O-7k7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"Saya suka makan bakso\",\n",
        "    \"Bakso enak dan lezat\",\n",
        "    \"Makanan favorit saya adalah nasi goreng\",\n",
        "    \"Nasi goreng pedas adalah makanan favorit saya\",\n",
        "    \"Saya suka makanan manis seperti es krim\",\n",
        "]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)"
      ],
      "metadata": {
        "id": "EkobrBUl7ou2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Di sini, TF-IDF (Term Frequency-Inverse Document Frequency) menghitung seberapa sering sebuah kata muncul dalam sebuah dokumen, lalu dibandingkan dengan seberapa sering kata tersebut muncul di seluruh koleksi dokumen. Ini membantu untuk menemukan kata-kata yang penting dalam sebuah dokumen."
      ],
      "metadata": {
        "id": "ZTHa4AChFvxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Melihat vocabulary (kata unik) yang dihasilkan oleh TfidfVectorizer"
      ],
      "metadata": {
        "id": "Z792uyDJ_Ct3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocabulary:\", tfidf_vectorizer.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXhix9F87osL",
        "outputId": "6b2c8732-79b6-4c0d-d09b-26a0fd91b360"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'saya': 14, 'suka': 16, 'makan': 9, 'bakso': 1, 'enak': 3, 'dan': 2, 'lezat': 8, 'makanan': 10, 'favorit': 5, 'adalah': 0, 'nasi': 12, 'goreng': 6, 'pedas': 13, 'manis': 11, 'seperti': 15, 'es': 4, 'krim': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jadi, dengan menggunakan TfidfVectorizer dari Scikit-learn, kita dapat mengonversi dokumen teks menjadi representasi TF-IDF, yang berguna untuk menganalisis kepentingan relatif dari kata-kata dalam setiap dokumen."
      ],
      "metadata": {
        "id": "mQcVpq2RGI6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Melihat hasil dari TF-IDF matrix dalam bentuk array"
      ],
      "metadata": {
        "id": "XMPx_vzg_ESK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_85gGQi7opp",
        "outputId": "68fe8cca-d5a4-458a-e342-51c9f6852581"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix:\n",
            "[[0.         0.49851188 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.61789262 0.         0.\n",
            "  0.         0.         0.34810993 0.         0.49851188]\n",
            " [0.         0.42224214 0.52335825 0.52335825 0.         0.\n",
            "  0.         0.         0.52335825 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.43951606 0.         0.         0.         0.         0.43951606\n",
            "  0.43951606 0.         0.         0.         0.36483803 0.\n",
            "  0.43951606 0.         0.30691325 0.         0.        ]\n",
            " [0.38596041 0.         0.         0.         0.         0.38596041\n",
            "  0.38596041 0.         0.         0.         0.320382   0.\n",
            "  0.38596041 0.47838798 0.26951544 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.42966246 0.\n",
            "  0.         0.42966246 0.         0.         0.28774996 0.42966246\n",
            "  0.         0.         0.24206433 0.42966246 0.34664897]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dengan TF-IDF Matrix ini, kita bisa melihat kata-kata yang paling penting dalam setiap dokumen berdasarkan konteksnya. Semakin tinggi nilai dalam tabel, semakin penting kata tersebut pada dokumen tersebut."
      ],
      "metadata": {
        "id": "moibxevBGKvM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words (BoW)\n",
        "\n",
        "Bag of Words (BoW) adalah pendekatan sederhana dalam pemrosesan teks yang mengubah teks menjadi representasi numerik. Ide dasarnya adalah kita menganggap setiap dokumen sebagai \"tas\" (bag) kata-kata dan hanya peduli tentang keberadaan kata-kata dalam dokumen tersebut, bukan urutan atau konteksnya. Kemudian, untuk setiap dokumen, kita hitung berapa kali setiap kata muncul.\n",
        "\n",
        "Hasilnya adalah matriks, yakni setiap baris mewakili sebuah dokumen dan setiap kolom mewakili kata-kata unik dalam seluruh kumpulan dokumen. Dengan cara ini, BoW memungkinkan kita mengukur kemunculan kata-kata dalam teks secara numerik, yang dapat digunakan untuk berbagai analisis teks, yakni klasifikasi dokumen, analisis sentimen, dan banyak lagi."
      ],
      "metadata": {
        "id": "racJVj4RGTdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inisialisasi objek CountVectorizer"
      ],
      "metadata": {
        "id": "pnKstfmZIW0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"Ini adalah contoh dokumen pertama.\",\n",
        "    \"Ini adalah dokumen kedua.\",\n",
        "    \"Ini adalah dokumen ketiga.\",\n",
        "    \"Ini adalah contoh contoh contoh.\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()"
      ],
      "metadata": {
        "id": "WQsnFqw-7ocl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dengan menggunakan CountVectorizer, kita bisa mengonversi teks menjadi representasi numerik yang dapat diproses lebih lanjut oleh algoritma pembelajaran mesin atau analisis statistik. Ini memungkinkan kita untuk melakukan berbagai analisis dan pemrosesan teks dengan menggunakan teknik-teknik pemrosesan bahasa alami."
      ],
      "metadata": {
        "id": "eOkisOgYJm-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Melakukan fitting dan transformasi pada data teks menggunakan CountVectorizer. Proses ini akan menghitung frekuensi kemunculan setiap kata dalam setiap dokumen."
      ],
      "metadata": {
        "id": "KK2-YE8qIoaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow_matrix = vectorizer.fit_transform(documents)"
      ],
      "metadata": {
        "id": "-6R0d6nGIoCC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setelah transformasi, kita mendapatkan matriks Bag of Words (BoW)."
      ],
      "metadata": {
        "id": "YbDMq9UlIyiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow_matrix.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCQMoWw7I1MO",
        "outputId": "a35b0d44-e921-4f0d-994d-0e8e6be16e13"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 1, 0, 0, 1],\n",
              "       [1, 0, 1, 1, 1, 0, 0],\n",
              "       [1, 0, 1, 1, 0, 1, 0],\n",
              "       [1, 3, 0, 1, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matriks Bag of Words (BoW) merupakan representasi numerik dari teks. Matriks ini berisi jumlah kemunculan setiap kata dalam setiap dokumen."
      ],
      "metadata": {
        "id": "MbCagcttJJWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kita juga mendapatkan daftar fitur (kata-kata) yang dihasilkan oleh CountVectorizer."
      ],
      "metadata": {
        "id": "vAyDkjL0I4wG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "AZtWEyrgJVk5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hasilnya, kita mencetak matriks BoW beserta daftar fitur yang dihasilkan."
      ],
      "metadata": {
        "id": "mSfpyn-7JXub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Matriks BoW:\")\n",
        "print(bow_matrix.toarray())\n",
        "\n",
        "print(\"\\nDaftar Fitur:\")\n",
        "print(features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO9iCzuEJYCV",
        "outputId": "d683a3ac-93ea-45b3-cd55-09df43d97239"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriks BoW:\n",
            "[[1 1 1 1 0 0 1]\n",
            " [1 0 1 1 1 0 0]\n",
            " [1 0 1 1 0 1 0]\n",
            " [1 3 0 1 0 0 0]]\n",
            "\n",
            "Daftar Fitur:\n",
            "['adalah' 'contoh' 'dokumen' 'ini' 'kedua' 'ketiga' 'pertama']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matriks Bag of Words (BoW) adalah representasi numerik dari teks yang menunjukkan jumlah kemunculan setiap kata dalam setiap dokumen. Pada contoh ini, matriks BoW memiliki 4 baris yang mewakili 4 dokumen dan 7 kolom yang mewakili 7 kata unik dalam teks.\n",
        "\n",
        "Misalnya, dalam dokumen pertama, kata 'adalah', 'contoh', 'dokumen', dan 'ini' muncul masing-masing 1 kali, sementara kata 'kedua', 'ketiga', dan 'pertama' tidak muncul. Daftar fitur adalah daftar kata-kata unik yang diurutkan secara alfabetis."
      ],
      "metadata": {
        "id": "LMjuwX-fJee4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-gram"
      ],
      "metadata": {
        "id": "2ThMq0eaKGWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"Saya suka makan bakso enak di warung dekat rumah.\",\n",
        "    \"Nasi goreng adalah salah satu makanan favorit saya.\",\n",
        "    \"Es krim coklat sangat lezat dan menyegarkan.\",\n",
        "    \"Saat hari hujan, saya suka minum teh hangat.\",\n",
        "    \"Pemandangan pegunungan di pagi hari sangat indah.\",\n",
        "    \"Bola basket adalah olahraga favorit saya sejak kecil.\"\n",
        "]"
      ],
      "metadata": {
        "id": "JpQJwGosKRln"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Iterasi melalui setiap kalimat dalam daftar kalimat."
      ],
      "metadata": {
        "id": "7cnQ11JTKSvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sentences:\n",
        "  words = sentence.split()\n",
        "  unigrams = list(ngrams(words, 1))\n",
        "  bigrams = list(ngrams(words, 2))\n",
        "  trigrams = list(ngrams(words, 3))"
      ],
      "metadata": {
        "id": "9LzNByvOKTAS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cetak hasil untuk setiap kalimat, termasuk unigram, bigram, dan trigram."
      ],
      "metadata": {
        "id": "QmQFsxTcKkSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Kalimat:\", sentence)\n",
        "print(\"1-gram:\")\n",
        "\n",
        "for gram in unigrams:\n",
        "    print(gram)\n",
        "print(\"\\n2-gram:\")\n",
        "\n",
        "for gram in bigrams:\n",
        "    print(gram)\n",
        "print(\"\\n3-gram:\")\n",
        "\n",
        "for gram in trigrams:\n",
        "    print(gram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP8RUz8MKkCB",
        "outputId": "51677880-1ecd-4e69-96c0-25a5f1074468"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kalimat: Bola basket adalah olahraga favorit saya sejak kecil.\n",
            "1-gram:\n",
            "('Bola',)\n",
            "('basket',)\n",
            "('adalah',)\n",
            "('olahraga',)\n",
            "('favorit',)\n",
            "('saya',)\n",
            "('sejak',)\n",
            "('kecil.',)\n",
            "\n",
            "2-gram:\n",
            "('Bola', 'basket')\n",
            "('basket', 'adalah')\n",
            "('adalah', 'olahraga')\n",
            "('olahraga', 'favorit')\n",
            "('favorit', 'saya')\n",
            "('saya', 'sejak')\n",
            "('sejak', 'kecil.')\n",
            "\n",
            "3-gram:\n",
            "('Bola', 'basket', 'adalah')\n",
            "('basket', 'adalah', 'olahraga')\n",
            "('adalah', 'olahraga', 'favorit')\n",
            "('olahraga', 'favorit', 'saya')\n",
            "('favorit', 'saya', 'sejak')\n",
            "('saya', 'sejak', 'kecil.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dengan tahapan ini, kita dapat membagi setiap kalimat menjadi unigram, bigram, dan trigram, serta menampilkan hasilnya. Dengan demikian, kita bisa melihat bahwa kata-kata pada teks tersebut berhubungan satu sama lain dalam berbagai tingkatan."
      ],
      "metadata": {
        "id": "cYeoRX-kKzEL"
      }
    }
  ]
}